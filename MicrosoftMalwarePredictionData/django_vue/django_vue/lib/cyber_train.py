import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import time
import psutil
import csv
import pandas as pd
from system_status import *

class CyberNN(nn.Module):

    def __init__(self, first_layer_size):
        super().__init__()
        second_layer_size = round(first_layer_size)
        third_layer_size = round(second_layer_size)
        self.fc1 = nn.Linear(in_features=first_layer_size, out_features=second_layer_size)
        self.fc2 = nn.Linear(in_features=second_layer_size, out_features=third_layer_size)
        self.out = nn.Linear(in_features=third_layer_size, out_features=1)

    def forward(self, t):
        t = t.flatten(start_dim=1)
        print(f'flattend tensor: {t}')
        t = F.relu(self.fc1(t))
        t = F.relu(self.fc2(t))
        t = self.out(t)
        return t


class Agent:
    def __init__(self, strategy, num_actions, device):
        self.current_step = 0
        self.strategy = strategy
        self.num_actions = num_actions
        self.device = device

    def select_action(self, state, policy_net):
        rate = self.strategy.get_exploration_rate(self.current_step)
        self.current_step += 1
        
        if rate > random.random():
            action = random.randrange(self.num_actions)
            return torch.tensor([action]).to(self.device) # explore
        else:
            with torch.no_grad():
                return policy_net(state).argmax(dim=1).to(self.device) #item() # exploit

class EpsilonGreedyStrategy:
    def __init__(self, start, end, decay):
        self.start = start
        self.end = end
        self.decay = decay

    def get_exploration_rate(self, current_step):
        return self.end + (self.start - self.end) * math.exp(-1. * current_step * self.decay)

class QValues:
    
    @staticmethod
    def get_current(policy_net, states, classification):
        output = policy_net(states)
        return output.gather(dim=1, index=classification.unsqueeze(-1))

    @staticmethod
    def get_next(target_net,states):
        return target_net(states).max(dim=1)[0]

class ReplayMemory:

    def __init__(self, capacity):
            self.capacity = capacity
            self.memory = []
            self.push_count = 0


        def push(self, experience):
            if len(self.memory) < self.capacity:
                self.memory.append(experience)
            else:
                self.memory[self.push_count % self.capacity] = experience

            self.push_count += 1

        def sample(self, batch_size):
            return random.sample(self.memory, batch_size)


        def can_provide_sample(self, batch_size):
            return len(self.memory) >= batch_size

class OfflineDQN:
    """
    Offline DQN learnng. Data is saved in a CSV file which is then used to train.
    Data is of type Dataframe when extracted therefore need to be converted to tensors.
    """

    def __init__(self,nn_data,learning_rate,batch_size,memory,episodes,strategy,agent,optimizer,cyber_policy_nn,target_policy_nn):
        self.INPUT_LAYER_SIZE = 13
        self.nn_data = nn_data
        self.learning_rate = learning_rate
        self.batch_size = batch_size
        self.memory = memory
        self.episodes = episodes 
        self.strategy = EpsilonGreedyStrategy(1,0.01,0.00001)
        self.agent = Agent(self.strategy,2,None)
        self.optimizer = optim.Adam(params=cyber_nn.parameters(), self.learning_rate=0.001)
        self.cyber_policy_nn = CyberNN(self.INPUT_LAYER_SIZE)
        self.cyber_target_nn = CyberNN(self.INPUT_LAYER_SIZE)

    def dataframe_to_tensor(self):
        torch_tensor = torch.tensor(self.nn_data.values)
        return torch_tensor

    def learn(self):
        training_data = self.dataframe_to_tensor()
        for episode in range(self.episodes)
            state = training_data[episode]
            next_state = training_data[episode+1]
            action = self.agent.select_action(state, self.cyber_policy_nn)



system_stats = SystemStatus(True)
INPUT_LAYER_SIZE = 13
NUMBER_OF_EPISODES = 1000
BATCH_SIZE = 32
MEMORY = None
CLASSIFICATION = 0
cyber_policy_nn = CyberNN(INPUT_LAYER_SIZE)
cyber_target_nn = CyberNN(INPUT_LAYER_SIZE)
memory = ReplayMemory(1000)
optimizer = optim.Adam(params=cyber_nn.parameters(), lr=0.001)

# data = pd.read_csv("cybernn.csv") 

with open('cybernn.csv','a',newline='') as file:
    fieldnames = ['cpu_percent0','cpu_percent1','cpu_percent2','cpu_percent3','cpu_time_percent0','cpu_time_percent1','cpu_time_percent2','cpu_frequency','cpu_avg_load0','cpu_avg_load1','cpu_avg_load2','network_packets_sent','network_packets_received']
    writer = csv.DictWriter(file,fieldnames=fieldnames)
    writer.writeheader()

    for episode in range(NUMBER_OF_EPISODES):
        network_packets_sent = 0
        network_packets_recv = 0
        initial_packets_count = system_stats.get_network_packets_sent()
        initial_recv_packets_count = system_stats.get_network_packets_received()

        cpu_percent = system_stats.get_cpu_percent()
        cpu_time_percent = system_stats.get_cpu_time_percent()
        cpu_frequency = system_stats.get_cpu_frequency()
        cpu_avg_load = system_stats.get_cpu_avg_load()
        print(0,torch.tensor(psutil.cpu_percent(interval=None, percpu=True)))
        print(1,cpu_percent)
        print(2,cpu_time_percent)
        print(3,cpu_frequency)
        print(4,cpu_avg_load)
        time.sleep(60)
        network_packets_sent = system_stats.get_network_packets_sent() - initial_packets_count
        network_packets_recv = system_stats.get_network_packets_received() - initial_recv_packets_count
        print(5,network_packets_sent)
        nn_input = torch.cat([cpu_percent,cpu_time_percent,cpu_frequency,cpu_avg_load,network_packets_sent,network_packets_recv])
        
        writer.writerow({'cpu_percent0':cpu_percent[0].item(),
                        'cpu_percent1':cpu_percent[1].item(),
                        'cpu_percent2':cpu_percent[2].item(),
                        'cpu_percent3':cpu_percent[3].item(),
                        'cpu_time_percent0':cpu_time_percent[0].item(),
                        'cpu_time_percent1':cpu_time_percent[1].item(),
                        'cpu_time_percent2':cpu_time_percent[2].item(),
                        'cpu_frequency':cpu_frequency.item(),
                        'cpu_avg_load0':cpu_avg_load[0].item(),
                        'cpu_avg_load1':cpu_avg_load[1].item(),
                        'cpu_avg_load2':cpu_avg_load[2].item(),
                        'network_packets_sent':network_packets_sent.item(),
                        'network_packets_received':network_packets_recv.item()})
        print(f'nn input: {nn_input}')
        print(f'i: {i}')

        if episode > 0:
            next_state = state
        state = nn_input
        action = agent.select_action(state, cyber_policy_nn)
        
        if memory.can_provide_sample(batch_size):
            experiences = memory.sample(batch_size)
            states, actions, rewards, next_states = extract_tensors(experiences)
            current_q_values = QValues.get_current(cyber_policy_nn, states, actions)
            next_q_values = QValues.get_next(cyber_target_nn, next_states)
            target_q_values = (next_q_values * discount_rate_gamma) + rewards
        # cyber_nn(nn_input)

        # loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))
        # optimizer.zero_grad()
        # loss.backward()
        # optimizer.step()
        
print("Done running NN")

